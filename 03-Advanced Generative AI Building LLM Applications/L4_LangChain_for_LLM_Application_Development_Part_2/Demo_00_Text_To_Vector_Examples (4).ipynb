{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfaf292b-5358-4022-ae08-9faf2cd92f5f",
   "metadata": {},
   "source": [
    "# **Text Processing and Feature Extraction in NLP**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd64c2a-c887-4c36-85c9-e61e4104a160",
   "metadata": {},
   "source": [
    "This guide walks through various text processing techniques in NLP, including tokenization, vectorization, one-hot encoding, and word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e1de0b-99ca-4a13-9bb5-ff7115919417",
   "metadata": {},
   "source": [
    "**Steps to be followed:**\n",
    "1. Tokenization\n",
    "2. Bag of Words (BoW) Vectorization\n",
    "3. One-Hot Encoding\n",
    "4. Word Embeddings (Word2Vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540af60f-ff79-4fba-9522-e9960f5726de",
   "metadata": {},
   "source": [
    "## __Step 1: Tokenization__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d35dc8-217d-4873-8a99-a35048d89b10",
   "metadata": {},
   "source": [
    "What is Tokenization?\n",
    "\n",
    "Tokenization is the process of breaking text into smaller units (tokens), such as words or subwords.\n",
    "This helps in text analysis, NLP preprocessing, and training ML models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "91b38c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = [\n",
    "    \"AI is ai ai transforming the world.\",\n",
    "    \"Natural language processing is a a subset of AI.\",\n",
    "    \"Deep learning and machine learning are popular AI techniques.\",\n",
    "    \"AI applications are diverse and growing rapidly.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78b9dd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02f281a8-aac4-4e5c-a552-8e88d1db0dfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AI', 'is', 'transforming', 'the', 'world', '.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(text_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b41bd97-e1bd-4302-b622-5ca7cda04698",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['AI', 'is', 'transforming', 'the', 'world', '.'],\n",
       " ['Natural', 'language', 'processing', 'is', 'a', 'subset', 'of', 'AI', '.'],\n",
       " ['Deep',\n",
       "  'learning',\n",
       "  'and',\n",
       "  'machine',\n",
       "  'learning',\n",
       "  'are',\n",
       "  'popular',\n",
       "  'AI',\n",
       "  'techniques',\n",
       "  '.'],\n",
       " ['AI', 'applications', 'are', 'diverse', 'and', 'growing', 'rapidly', '.']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the text data\n",
    "tokenized_data = [word_tokenize(sentence) for sentence in text_data]\n",
    "tokenized_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ab980f-646b-47f1-ab44-3873a356e9f6",
   "metadata": {},
   "source": [
    "\n",
    "## __Step 2: Bag of Words (BoW) Vectorization__\n",
    "\n",
    "What is BoW?\n",
    "\n",
    "Bag of Words (BoW) converts text into numerical form.\n",
    "It counts word occurrences across sentences/documents.\n",
    "\n",
    "\n",
    "frequency:\n",
    "features = vocabulory = 5\n",
    "cat sat mat\n",
    "cat hat\n",
    "cat cat rat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1ae11efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize the CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the text data\n",
    "vectorized_data = vectorizer.fit_transform(text_data)\n",
    "\n",
    "#dir(vectorizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8cc6ba65-78b5-4c0c-bf47-f2ac7dd53205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9abd3e73-aeff-4d20-a734-7a58378df6af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ai', 'and', 'applications', 'are', 'deep', 'diverse', 'growing',\n",
       "       'is', 'language', 'learning', 'machine', 'natural', 'of',\n",
       "       'popular', 'processing', 'rapidly', 'subset', 'techniques', 'the',\n",
       "       'transforming', 'world'], dtype=object)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the feature names (tokens)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4a5b1e19-25b9-4cda-b8f8-9bd8a6f101d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ai': 0,\n",
       " 'is': 7,\n",
       " 'transforming': 19,\n",
       " 'the': 18,\n",
       " 'world': 20,\n",
       " 'natural': 11,\n",
       " 'language': 8,\n",
       " 'processing': 14,\n",
       " 'subset': 16,\n",
       " 'of': 12,\n",
       " 'deep': 4,\n",
       " 'learning': 9,\n",
       " 'and': 1,\n",
       " 'machine': 10,\n",
       " 'are': 3,\n",
       " 'popular': 13,\n",
       " 'techniques': 17,\n",
       " 'applications': 2,\n",
       " 'diverse': 5,\n",
       " 'growing': 6,\n",
       " 'rapidly': 15}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e4beedf7-7038-43e7-a736-0eb8225388cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(vectorized_data.toarray(), columns= vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7f4451e0-eff3-44fc-bc9f-758410e330fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Text\"] = text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8f92739a-77c2-48ee-b740-af6dcbff2cc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ai</th>\n",
       "      <th>and</th>\n",
       "      <th>applications</th>\n",
       "      <th>are</th>\n",
       "      <th>deep</th>\n",
       "      <th>diverse</th>\n",
       "      <th>growing</th>\n",
       "      <th>is</th>\n",
       "      <th>language</th>\n",
       "      <th>learning</th>\n",
       "      <th>...</th>\n",
       "      <th>natural</th>\n",
       "      <th>of</th>\n",
       "      <th>popular</th>\n",
       "      <th>processing</th>\n",
       "      <th>rapidly</th>\n",
       "      <th>subset</th>\n",
       "      <th>techniques</th>\n",
       "      <th>the</th>\n",
       "      <th>transforming</th>\n",
       "      <th>world</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ai  and  applications  are  deep  diverse  growing  is  language  learning  \\\n",
       "0   1    0             0    0     0        0        0   1         0         0   \n",
       "1   1    0             0    0     0        0        0   1         1         0   \n",
       "2   1    1             0    1     1        0        0   0         0         2   \n",
       "3   1    1             1    1     0        1        1   0         0         0   \n",
       "\n",
       "   ...  natural  of  popular  processing  rapidly  subset  techniques  the  \\\n",
       "0  ...        0   0        0           0        0       0           0    1   \n",
       "1  ...        1   1        0           1        0       1           0    0   \n",
       "2  ...        0   0        1           0        0       0           1    0   \n",
       "3  ...        0   0        0           0        1       0           0    0   \n",
       "\n",
       "   transforming  world  \n",
       "0             1      1  \n",
       "1             0      0  \n",
       "2             0      0  \n",
       "3             0      0  \n",
       "\n",
       "[4 rows x 21 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df #vocabulory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c179c03-d0a9-43a6-8bca-4090435838c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert to array for better visualization\n",
    "vector_array = vectorized_data.toarray()\n",
    "\n",
    "feature_names, vector_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7214cdd-7284-4a05-ae8e-74c53b6b9b0c",
   "metadata": {},
   "source": [
    "## __Step 3: One-Hot Encoding__\n",
    "\n",
    "What is One-Hot Encoding?\n",
    "\n",
    "One-Hot Encoding represents text as binary vectors.\n",
    "Each word gets a unique index and is encoded as 0s and 1s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ef5ed7-4c98-4e71-b5f9-de70a0287500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gender  f    m   t\n",
    "\n",
    "# f =1      1    0   0\n",
    "# m =2     0    1   0\n",
    "# t =3      0    0   1\n",
    "\n",
    "# categorical--\n",
    "# education\n",
    "# ug=1\n",
    "# pg=2\n",
    "# phd\n",
    "\n",
    "# nominal---one hot encoder\n",
    "# ordinal--label encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "41c5d646-3efc-427b-9ce3-14c081ded7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.preprocessing import OneHotEncoder #nominal categorical\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5434f5f6-3aa3-4bed-8466-f7996541a71e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample Text Dataset\n",
    "text_data = [\n",
    "    \"AI is transforming the world.\",\n",
    "    \"Natural language processing is a subset of AI.\",\n",
    "    \"Deep learning and machine learning are popular AI techniques.\",\n",
    "    \"AI applications are diverse and growing rapidly.\",\n",
    "    \"anu's book\"\n",
    "]\n",
    "len(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b104029c-39de-4782-8435-c1118c688ecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'like', 'it', '!']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word_tokenize(\"i like it!\")\n",
    "# \\n\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7d1bed0d-934b-4a64-876e-324a884f51fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ai is transforming the world.'],\n",
       " ['natural language processing is a subset of ai.'],\n",
       " ['deep learning and machine learning are popular ai techniques.'],\n",
       " ['ai applications are diverse and growing rapidly.']]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the text data\n",
    "tokenized_data = [sent_tokenize(sentence.lower()) for sentence in text_data]\n",
    "tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb406b4-71fe-485e-91d0-39f28091e55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM- tokens\n",
    "# AI==numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ae3738d-fadf-401b-9f47-cb5ccc73810d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ai', 'is', 'transforming', 'the', 'world', '.'],\n",
       " ['natural', 'language', 'processing', 'is', 'a', 'subset', 'of', 'ai', '.'],\n",
       " ['deep',\n",
       "  'learning',\n",
       "  'and',\n",
       "  'machine',\n",
       "  'learning',\n",
       "  'are',\n",
       "  'popular',\n",
       "  'ai',\n",
       "  'techniques',\n",
       "  '.'],\n",
       " ['ai', 'applications', 'are', 'diverse', 'and', 'growing', 'rapidly', '.']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the text data\n",
    "tokenized_data = [word_tokenize(sentence.lower()) for sentence in text_data]\n",
    "tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d69771e8-b8c0-4cf0-b26a-07e77bf23e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_token = []\n",
    "for i in tokenized_data:\n",
    "    for j in i:\n",
    "        list_token.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f19a8f0-bccd-4f55-bc1a-af7b6354e5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c27fa4c4-b282-402f-b5fe-ac195d0f9609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flatten the list of tokenized words\n",
    "flattened_tokens = [word for sentence in tokenized_data for word in sentence]\n",
    "type(flattened_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a956976f-1fe5-4802-9b9a-be3ee993c028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(flattened_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7cb3d2ed-159d-4641-8090-b2cad1f5c78b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 2}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{1,1,1,1,2,2,2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a81864a0-fbf2-44db-b40b-fb2e3daa389f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # LLM= vocab=10000\n",
    "\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2-large\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1624cca-8e9b-4487-9f92-9a87fa01ab9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f68ccc10-152f-4080-b28f-3003e5064e4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['popular',\n",
       " 'transforming',\n",
       " 'world',\n",
       " 'growing',\n",
       " 'the',\n",
       " 'ai',\n",
       " 'a',\n",
       " 'diverse',\n",
       " 'is',\n",
       " '.',\n",
       " 'are',\n",
       " 'deep',\n",
       " 'machine',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'subset',\n",
       " 'applications',\n",
       " 'and',\n",
       " 'techniques',\n",
       " 'learning',\n",
       " 'of',\n",
       " 'natural',\n",
       " 'rapidly']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a vocabulary of unique words\n",
    "vocabulary = list(set(flattened_tokens))\n",
    "(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e64ba2e9-c064-474b-bfeb-d2ec83f37bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.array(vocabulary).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7bbffae3-a05b-45f5-b9f8-0bf53319cd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OneHotEncoder with the vocabulary\n",
    "encoder = OneHotEncoder()\n",
    "x1 = encoder.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "01997673-ee80-4359-b622-962a5b9b584a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1da6ef9e-5b3b-4d28-841d-ac5baf6dfa1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['x0_.', 'x0_a', 'x0_ai', 'x0_and', 'x0_applications', 'x0_are',\n",
       "       'x0_deep', 'x0_diverse', 'x0_growing', 'x0_is', 'x0_language',\n",
       "       'x0_learning', 'x0_machine', 'x0_natural', 'x0_of', 'x0_popular',\n",
       "       'x0_processing', 'x0_rapidly', 'x0_subset', 'x0_techniques',\n",
       "       'x0_the', 'x0_transforming', 'x0_world'], dtype=object)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b8e0be41-8b19-42df-9d2e-f315357896ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0_.</th>\n",
       "      <th>x0_a</th>\n",
       "      <th>x0_ai</th>\n",
       "      <th>x0_and</th>\n",
       "      <th>x0_applications</th>\n",
       "      <th>x0_are</th>\n",
       "      <th>x0_deep</th>\n",
       "      <th>x0_diverse</th>\n",
       "      <th>x0_growing</th>\n",
       "      <th>x0_is</th>\n",
       "      <th>...</th>\n",
       "      <th>x0_natural</th>\n",
       "      <th>x0_of</th>\n",
       "      <th>x0_popular</th>\n",
       "      <th>x0_processing</th>\n",
       "      <th>x0_rapidly</th>\n",
       "      <th>x0_subset</th>\n",
       "      <th>x0_techniques</th>\n",
       "      <th>x0_the</th>\n",
       "      <th>x0_transforming</th>\n",
       "      <th>x0_world</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    x0_.  x0_a  x0_ai  x0_and  x0_applications  x0_are  x0_deep  x0_diverse  \\\n",
       "0    0.0   0.0    0.0     0.0              0.0     0.0      0.0         0.0   \n",
       "1    0.0   0.0    0.0     0.0              0.0     0.0      0.0         0.0   \n",
       "2    0.0   0.0    0.0     0.0              0.0     0.0      0.0         0.0   \n",
       "3    0.0   0.0    0.0     0.0              0.0     0.0      0.0         0.0   \n",
       "4    0.0   0.0    0.0     0.0              0.0     0.0      0.0         0.0   \n",
       "5    0.0   0.0    1.0     0.0              0.0     0.0      0.0         0.0   \n",
       "6    0.0   1.0    0.0     0.0              0.0     0.0      0.0         0.0   \n",
       "7    0.0   0.0    0.0     0.0              0.0     0.0      0.0         1.0   \n",
       "8    0.0   0.0    0.0     0.0              0.0     0.0      0.0         0.0   \n",
       "9    1.0   0.0    0.0     0.0              0.0     0.0      0.0         0.0   \n",
       "10   0.0   0.0    0.0     0.0              0.0     1.0      0.0         0.0   \n",
       "11   0.0   0.0    0.0     0.0              0.0     0.0      1.0         0.0   \n",
       "12   0.0   0.0    0.0     0.0              0.0     0.0      0.0         0.0   \n",
       "13   0.0   0.0    0.0     0.0              0.0     0.0      0.0         0.0   \n",
       "14   0.0   0.0    0.0     0.0              0.0     0.0      0.0         0.0   \n",
       "15   0.0   0.0    0.0     0.0              0.0     0.0      0.0         0.0   \n",
       "16   0.0   0.0    0.0     0.0              1.0     0.0      0.0         0.0   \n",
       "17   0.0   0.0    0.0     1.0              0.0     0.0      0.0         0.0   \n",
       "18   0.0   0.0    0.0     0.0              0.0     0.0      0.0         0.0   \n",
       "19   0.0   0.0    0.0     0.0              0.0     0.0      0.0         0.0   \n",
       "20   0.0   0.0    0.0     0.0              0.0     0.0      0.0         0.0   \n",
       "21   0.0   0.0    0.0     0.0              0.0     0.0      0.0         0.0   \n",
       "22   0.0   0.0    0.0     0.0              0.0     0.0      0.0         0.0   \n",
       "\n",
       "    x0_growing  x0_is  ...  x0_natural  x0_of  x0_popular  x0_processing  \\\n",
       "0          0.0    0.0  ...         0.0    0.0         1.0            0.0   \n",
       "1          0.0    0.0  ...         0.0    0.0         0.0            0.0   \n",
       "2          0.0    0.0  ...         0.0    0.0         0.0            0.0   \n",
       "3          1.0    0.0  ...         0.0    0.0         0.0            0.0   \n",
       "4          0.0    0.0  ...         0.0    0.0         0.0            0.0   \n",
       "5          0.0    0.0  ...         0.0    0.0         0.0            0.0   \n",
       "6          0.0    0.0  ...         0.0    0.0         0.0            0.0   \n",
       "7          0.0    0.0  ...         0.0    0.0         0.0            0.0   \n",
       "8          0.0    1.0  ...         0.0    0.0         0.0            0.0   \n",
       "9          0.0    0.0  ...         0.0    0.0         0.0            0.0   \n",
       "10         0.0    0.0  ...         0.0    0.0         0.0            0.0   \n",
       "11         0.0    0.0  ...         0.0    0.0         0.0            0.0   \n",
       "12         0.0    0.0  ...         0.0    0.0         0.0            0.0   \n",
       "13         0.0    0.0  ...         0.0    0.0         0.0            0.0   \n",
       "14         0.0    0.0  ...         0.0    0.0         0.0            1.0   \n",
       "15         0.0    0.0  ...         0.0    0.0         0.0            0.0   \n",
       "16         0.0    0.0  ...         0.0    0.0         0.0            0.0   \n",
       "17         0.0    0.0  ...         0.0    0.0         0.0            0.0   \n",
       "18         0.0    0.0  ...         0.0    0.0         0.0            0.0   \n",
       "19         0.0    0.0  ...         0.0    0.0         0.0            0.0   \n",
       "20         0.0    0.0  ...         0.0    1.0         0.0            0.0   \n",
       "21         0.0    0.0  ...         1.0    0.0         0.0            0.0   \n",
       "22         0.0    0.0  ...         0.0    0.0         0.0            0.0   \n",
       "\n",
       "    x0_rapidly  x0_subset  x0_techniques  x0_the  x0_transforming  x0_world  \n",
       "0          0.0        0.0            0.0     0.0              0.0       0.0  \n",
       "1          0.0        0.0            0.0     0.0              1.0       0.0  \n",
       "2          0.0        0.0            0.0     0.0              0.0       1.0  \n",
       "3          0.0        0.0            0.0     0.0              0.0       0.0  \n",
       "4          0.0        0.0            0.0     1.0              0.0       0.0  \n",
       "5          0.0        0.0            0.0     0.0              0.0       0.0  \n",
       "6          0.0        0.0            0.0     0.0              0.0       0.0  \n",
       "7          0.0        0.0            0.0     0.0              0.0       0.0  \n",
       "8          0.0        0.0            0.0     0.0              0.0       0.0  \n",
       "9          0.0        0.0            0.0     0.0              0.0       0.0  \n",
       "10         0.0        0.0            0.0     0.0              0.0       0.0  \n",
       "11         0.0        0.0            0.0     0.0              0.0       0.0  \n",
       "12         0.0        0.0            0.0     0.0              0.0       0.0  \n",
       "13         0.0        0.0            0.0     0.0              0.0       0.0  \n",
       "14         0.0        0.0            0.0     0.0              0.0       0.0  \n",
       "15         0.0        1.0            0.0     0.0              0.0       0.0  \n",
       "16         0.0        0.0            0.0     0.0              0.0       0.0  \n",
       "17         0.0        0.0            0.0     0.0              0.0       0.0  \n",
       "18         0.0        0.0            1.0     0.0              0.0       0.0  \n",
       "19         0.0        0.0            0.0     0.0              0.0       0.0  \n",
       "20         0.0        0.0            0.0     0.0              0.0       0.0  \n",
       "21         0.0        0.0            0.0     0.0              0.0       0.0  \n",
       "22         1.0        0.0            0.0     0.0              0.0       0.0  \n",
       "\n",
       "[23 rows x 23 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(x1.toarray(), columns=encoder.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3859168-c116-4d95-9279-db6f77808ee1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ba3cf80d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['popular',\n",
       "  'transforming',\n",
       "  'world',\n",
       "  'growing',\n",
       "  'the',\n",
       "  'ai',\n",
       "  'a',\n",
       "  'diverse',\n",
       "  'is',\n",
       "  '.',\n",
       "  'are',\n",
       "  'deep',\n",
       "  'machine',\n",
       "  'language',\n",
       "  'processing',\n",
       "  'subset',\n",
       "  'applications',\n",
       "  'and',\n",
       "  'techniques',\n",
       "  'learning',\n",
       "  'of',\n",
       "  'natural',\n",
       "  'rapidly'],\n",
       " array([[[1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 1., 1., 1.]],\n",
       " \n",
       "        [[1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0.,\n",
       "          1., 0., 1., 0., 0., 0., 0.]],\n",
       " \n",
       "        [[1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 2., 1., 0., 0., 1.,\n",
       "          0., 0., 0., 1., 0., 0., 0.]],\n",
       " \n",
       "        [[1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 1., 0., 0., 0., 0., 0.]]]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Encode each sentence\n",
    "one_hot_encoded_data = []\n",
    "for sentence in tokenized_data:\n",
    "    encoded_sentence = encoder.transform(np.array(sentence).reshape(-1, 1))\n",
    "    one_hot_encoded_data.append(np.sum(encoded_sentence, axis=0))\n",
    "\n",
    "# Output the vocabulary and one-hot encoded data\n",
    "vocabulary, np.array(one_hot_encoded_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab24350",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.array(one_hot_encoded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3221fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "b[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afa4d53-b1c3-4bb5-b941-d7e6664fa828",
   "metadata": {},
   "source": [
    "## __Step 4: Word Embeddings (Word2Vec)__\n",
    "\n",
    "What is Word2Vec?\n",
    "\n",
    "Word2Vec converts words into dense vector representations.\n",
    "Similar words have similar vectors based on context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61573d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Sample Text Dataset\n",
    "text_data = [\n",
    "    \"AI is transforming the world.\",\n",
    "    \"Natural language processing is a subset of AI.\",\n",
    "    \"Deep learning and machine learning are popular AI techniques.\",\n",
    "    \"AI applications are diverse and growing rapidly.\"\n",
    "]\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenized_data = [word_tokenize(sentence.lower()) for sentence in text_data]\n",
    "\n",
    "# Train the Word2Vec model\n",
    "model = Word2Vec(sentences=tokenized_data, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Get the vocabulary\n",
    "vocabulary = list(model.wv.index_to_key)\n",
    "\n",
    "# Get word embeddings for the vocabulary\n",
    "word_embeddings = {word: model.wv[word] for word in vocabulary}\n",
    "\n",
    "# Example: Get embedding for the word 'ai'\n",
    "embedding_ai = word_embeddings['ai']\n",
    "\n",
    "vocabulary, embedding_ai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3d5bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "1\n",
    "2\n",
    "3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 [3.10]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
