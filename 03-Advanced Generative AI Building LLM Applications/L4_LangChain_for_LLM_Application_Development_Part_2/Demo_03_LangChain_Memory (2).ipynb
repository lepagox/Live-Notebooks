{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "snBYHptMnyJb",
   "metadata": {
    "id": "snBYHptMnyJb"
   },
   "source": [
    "# **Demo: LangChain Memory**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Dld3JlxbT53P",
   "metadata": {
    "id": "Dld3JlxbT53P"
   },
   "source": [
    "#__Description:__\n",
    "In this activity, you will learn how to use different types of memory in LangChain. Each type of memory serves a unique purpose and can be used in different scenarios depending on the requirements of your conversation model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cVXoe4RUn3O8",
   "metadata": {
    "id": "cVXoe4RUn3O8"
   },
   "source": [
    "# **Steps to Perform:**\n",
    "\n",
    "1. Import the Necessary Modules\n",
    "2. Initialize the Chat Model\n",
    "3. Define ConversationBufferMemory\n",
    "4. Define ConversationBufferWindowMemory with a Window Size of 1\n",
    "5. Define ConversationTokenBufferMemory with a Maximum Token Limit of 30\n",
    "6. Define ConversationSummaryBufferMemory with a Maximum Token Limit of 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eyRI2cM0UL1J",
   "metadata": {
    "id": "eyRI2cM0UL1J"
   },
   "source": [
    "# **Step 1: Import the Necessary Modules**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed4b7ae-a580-4cf2-9f28-228dc05896eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm+tool+memory\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f85e3c5a",
   "metadata": {
    "id": "f85e3c5a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory, ConversationBufferWindowMemory, ConversationTokenBufferMemory, ConversationSummaryBufferMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cFIuxtHkUTIc",
   "metadata": {
    "id": "cFIuxtHkUTIc"
   },
   "source": [
    "# **Step 2: Initialize the Chat Model**\n",
    "\n",
    "\n",
    "*   Initialize the chat model with a temperature of 0.0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d89f68a-3d5d-4b78-af87-5b0bea28d83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from secret_key import openapi_key\n",
    "import os\n",
    "#os.environ['OPENAI_API_KEY'] = openapi_key\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ab3e43e",
   "metadata": {
    "id": "3ab3e43e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_182/3662068896.py:1: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(temperature=0.0)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "llm = ChatOpenAI(temperature=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1H_MPaTsUZdr",
   "metadata": {
    "id": "1H_MPaTsUZdr"
   },
   "source": [
    "# **Step 3: Define ConversationBufferMemory**\n",
    "\n",
    "\n",
    "*   Utilize it in a ConversationChain for making predictions and printing the memory buffer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32525373",
   "metadata": {
    "id": "32525373",
    "outputId": "94f91c9d-0550-4f10-d677-c75aa61498c1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello Alex! It's nice to meet you. How can I assist you today?\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = ConversationBufferMemory()\n",
    "\n",
    "conversation = ConversationChain(llm=llm, memory = memory, verbose=False)\n",
    "\n",
    "conversation.predict(input=\"Hello, my name is Alex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "678e6d0c-3d89-44b0-89cf-9b3b32c295cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is Alex.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is my name?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2dc3e08d-640f-4cbd-9b2e-be5d01ec0727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The above output is \"Alex.\" Adding 10 to a name doesn\\'t make sense, so I\\'m not sure how to respond to that. Is there something else I can help you with?'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Add 10 to the above output\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c1dd605-d2c1-47bd-bb84-a7751d045784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sure! What string would you like me to add to \"Alex\"?'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"atleast can you add any string to it\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfc64ffe-6b7e-45ce-a41a-0860258a0ae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2 + 2 equals 4. Is there anything else you would like to know?'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is 2+2?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6a9388c-d0b9-4469-a460-f15c7fc5ba31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hello, my name is Alex\n",
      "AI: Hello Alex! It's nice to meet you. How can I assist you today?\n",
      "Human: What is my name?\n",
      "AI: Your name is Alex.\n",
      "Human: Add 10 to the above output\n",
      "AI: The above output is \"Alex.\" Adding 10 to a name doesn't make sense, so I'm not sure how to respond to that. Is there something else I can help you with?\n",
      "Human: atleast can you add any string to it\n",
      "AI: Sure! What string would you like me to add to \"Alex\"?\n",
      "Human: What is 2+2?\n",
      "AI: 2 + 2 equals 4. Is there anything else you would like to know?\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fHKrpuvIUfny",
   "metadata": {
    "id": "fHKrpuvIUfny"
   },
   "source": [
    "# **Step 4: Define ConversationBufferWindowMemory with a Window Size of 1**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3c406c55",
   "metadata": {
    "id": "3c406c55",
    "outputId": "dd708430-a4a1-4a89-d140-f144c40bcd9d",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hello\\nAI: How are you?\\nHuman: I'm good, thanks\\nAI: Great to hear!\"}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = ConversationBufferWindowMemory(k=3)\n",
    "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"How are you?\"})\n",
    "memory.save_context({\"input\": \"I'm good, thanks\"}, {\"output\": \"Great to hear!\"})\n",
    "# memory.save_context({\"input\": \"I'm good, thanks\"}, {\"output\": \"Great to hear!\"})\n",
    "# memory.save_context({\"input\": \"I'm good, thanks\"}, {\"output\": \"Great to hear!\"})\n",
    "# memory.save_context({\"input\": \"I'm good, thanks\"}, {\"output\": \"Great to hear!\"})\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b4a166b0-806a-4ad5-8ee7-e1964384ddee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How are you today?'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = ConversationBufferWindowMemory(k = 2)\n",
    "conversation = ConversationChain(llm=llm, memory = memory, verbose=False)\n",
    "conversation.predict(input=\"Hello\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c56f4527-fbb7-40c8-b530-211a275f0dbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Of course, Alex! During the monsoon season, it's best to avoid regions that experience heavy rainfall and potential flooding. I would recommend considering destinations that have a more temperate climate during this time, such as parts of Southeast Asia like Bali or Thailand. These areas typically have less rainfall during the monsoon season and offer beautiful landscapes and cultural experiences. Additionally, you may want to consider visiting places like the Mediterranean region or parts of South America, where the weather is more stable during this time of year. Let me know if you need more specific recommendations!\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"can you suggest me travel plan during monsoon season\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5044e80c-5eab-4675-a63a-98e3d5e9e3c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is Alex.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is my name?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a098dda5-3282-4990-866d-a1f51d6f8aa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The above output was \"Your name is Alex.\" Adding 10 to that would not make sense in this context. Is there anything else I can assist you with?'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Add 10 to the above output\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00aa0187-b00c-4c60-9604-b8bbf2bc3bce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, I do not have the ability to determine which season you asked about based on the current conversation. Could you please provide more context or clarify your question?\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"which season i asked\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945eae1f-f30b-41e4-a874-4f2bc6307932",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"which season i asked\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b206863f-a3c1-4391-889e-42326950ced0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Add 10 to the above output\n",
      "AI: The above output was \"Your name is Alex.\" Adding 10 to that would not make sense in this context. Is there anything else I can assist you with?\n",
      "Human: which season i asked\n",
      "AI: I'm sorry, I do not have the ability to determine which season you asked about based on the current conversation. Could you please provide more context or clarify your question?\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cTgW8BUtvk",
   "metadata": {
    "id": "07cTgW8BUtvk"
   },
   "source": [
    "# **Step 5: Define ConversationTokenBufferMemory with a Maximum Token Limit of 30**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e01a67ba",
   "metadata": {
    "id": "e01a67ba",
    "outputId": "7342f887-ab75-4eb9-9b6d-730085ab98fb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'AI: Fascinating!\\nHuman: AI Assistants are what?\\nAI: Impressive!'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=40, separators=[\"\\n\\n\"])\n",
    "memory.save_context({\"input\": \"Machine Learning is what?!\"}, {\"output\": \"Incredible!\"})\n",
    "memory.save_context({\"input\": \"Neural Networks are what?\"}, {\"output\": \"Fascinating!\"})\n",
    "memory.save_context({\"input\": \"AI Assistants are what?\"}, {\"output\": \"Impressive!\"})\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6b7abdfe-968c-4295-8419-1bd03f4d250f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello Alex! It's nice to meet you. How can I assist you today?\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation = ConversationChain(llm=llm, memory = memory, verbose=False)\n",
    "conversation.predict(input=\"Hello, my name is Alex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a9232c1e-70c7-41b6-a36f-75f651fd8bf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Of course! Here's a summary of your schedule for today: You have a meeting at 10am with your design team where you will need your sketch designs ready. From 11am to 2pm, you have time to work on your LangChain project, which should go smoothly because LangChain is a powerful tool. At 2pm, you have lunch at a sushi restaurant with a client who is flying in to meet you and understand the latest in AI. Don't forget to bring your tablet to show the latest LLM demo.\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"\"\"can you summarize this, There is a meeting at 10am with your design team. \\\n",
    "You will need your sketch designs ready. \\\n",
    "11am-2pm have time to work on your LangChain \\\n",
    "project which will go smoothly because Langchain is such a powerful tool. \\\n",
    "At 2pm, lunch at the sushi restaurant with a client who is flying \\\n",
    "in to meet you to understand the latest in AI. \\\n",
    "Be sure to bring your tablet to show the latest LLM demo.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e5db5290-431b-40e7-8bb1-3aa62486acdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Machine learning is a subset of artificial intelligence that involves the development of algorithms and statistical models that enable computers to learn from and make predictions or decisions based on data without being explicitly programmed. It uses techniques such as neural networks, decision trees, and support vector machines to analyze and interpret complex data sets. Machine learning is used in a wide range of applications, including image and speech recognition, medical diagnosis, financial forecasting, and autonomous vehicles.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Machine Learning is what?!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6fb9e3a3-97e4-461a-ac25-34080b3dfa04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: rephrase summary\n",
      "AI: Sure! Can you provide me with the summary you would like me to rephrase?\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "N5B6Y6g9U15e",
   "metadata": {
    "id": "N5B6Y6g9U15e"
   },
   "source": [
    "# **Step 6: Define ConversationSummaryBufferMemory with a Maximum Token Limit of 100**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "713bc684",
   "metadata": {
    "id": "713bc684",
    "outputId": "e781ab8b-d02d-4f2e-b04b-d14a75a82917"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_182/1589341142.py:8: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'history': \"System: The human greets the AI and asks what's up. The AI responds that it's working and the human inquires about the agenda for the day.\\nAI: There is a meeting at 10am with your design team. You will need your sketch designs ready. 11am-2pm have time to work on your LangChain project which will go smoothly because Langchain is such a powerful tool. At 2pm, lunch at the sushi restaurant with a client who is flying in to meet you to understand the latest in AI. Be sure to bring your tablet to show the latest LLM demo.\"}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schedule = \"There is a meeting at 10am with your design team. \\\n",
    "You will need your sketch designs ready. \\\n",
    "11am-2pm have time to work on your LangChain \\\n",
    "project which will go smoothly because Langchain is such a powerful tool. \\\n",
    "At 2pm, lunch at the sushi restaurant with a client who is flying \\\n",
    "in to meet you to understand the latest in AI. \\\n",
    "Be sure to bring your tablet to show the latest LLM demo.\"\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n",
    "memory.save_context({\"input\": \"Hi\"}, {\"output\": \"Hello\"})\n",
    "memory.save_context({\"input\": \"What's up?\"}, {\"output\": \"Not much, just working\"})\n",
    "memory.save_context({\"input\": \"What is on the agenda today?\"}, {\"output\": f\"{schedule}\"})\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5aa1d09e-4276-412c-abf0-9ff036623fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: There is a meeting at 10am with your design team. You will need your sketch designs ready. 11am-2pm have time to work on your LangChain project which will go smoothly because Langchain is such a powerful tool. At 2pm, lunch at the sushi restaurant with a client who is flying in to meet you to understand the latest in AI. Be sure to bring your tablet to show the latest LLM demo.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Thank you for the detailed schedule! I will make sure to have my sketch designs ready for the meeting with the design team at 10am. I'm looking forward to working on my LangChain project from 11am-2pm, as you mentioned LangChain is a powerful tool that will make the process smooth. I will definitely bring my tablet to the sushi restaurant at 2pm to show the latest LLM demo to the client flying in to learn about the latest in AI. I appreciate the heads up!\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=40)\n",
    "\n",
    "conversation = ConversationChain(llm=llm, memory = memory, verbose=True)\n",
    "\n",
    "conversation.predict(input=schedule)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2882864b-8564-4c0a-b132-8a95cfca544f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: The human provides a detailed schedule for the AI, including a meeting with the design team at 10am, working on the LangChain project from 11am-2pm, and a lunch meeting with a client at 2pm to discuss the latest in AI. The AI expresses gratitude for the schedule and confirms readiness for each event, acknowledging the power of LangChain and the importance of showcasing the latest LLM demo to the client.\n",
      "Human: What is 2+2?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2+2 equals 4.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is 2+2?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1a6fc734-a6bb-4d0e-bee7-0b259d507302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: The human provides a detailed schedule for the AI, including a meeting with the design team at 10am, working on the LangChain project from 11am-2pm, and a lunch meeting with a client at 2pm to discuss the latest in AI. The AI expresses gratitude for the schedule and confirms readiness for each event, acknowledging the power of LangChain and the importance of showcasing the latest LLM demo to the client.\n",
      "Human: What is 2+2?\n",
      "AI: 2+2 equals 4.\n",
      "Human: What is my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, I do not have that information.\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is my name?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d3ac27f6-d233-4db3-a443-5a2505b10bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: The human provides a detailed schedule for the AI, including a meeting with the design team at 10am, working on the LangChain project from 11am-2pm, and a lunch meeting with a client at 2pm to discuss the latest in AI. The AI expresses gratitude for the schedule and confirms readiness for each event, acknowledging the power of LangChain and the importance of showcasing the latest LLM demo to the client. The human then asks a simple math question, to which the AI correctly responds that 2+2 equals 4.\n",
      "Human: What is my name?\n",
      "AI: I'm sorry, I do not have that information.\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uxHfpP_fVBNB",
   "metadata": {
    "id": "uxHfpP_fVBNB"
   },
   "source": [
    "# **Conclusion**\n",
    "You learned how to use different types of memory in LangChain and how to implement and use ConversationBufferMemory, ConversationBufferWindowMemory, ConversationTokenBufferMemory, and ConversationSummaryBufferMemory. You’ve also understood how ConversationSummaryBufferMemory can handle larger pieces of information, such as a daily schedule. This knowledge will help you effectively utilize LangChain’s memory capabilities in your projects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db224b5-bfba-434e-bf99-f4fb57d924f2",
   "metadata": {
    "id": "4db224b5-bfba-434e-bf99-f4fb57d924f2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 [3.10]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
