{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cSNe5dGyCZlq"
   },
   "source": [
    "#**Demo: Building a Text Generation Pipeline with LangChain and Hugging Face's Flan T5 XXL Model**\n",
    "\n",
    "In this demo, you will learn how to create a Langchain HuggingFacePipeline for efficient text generation and dive into the creation of a Langchain chain to craft context-aware responses using a custom template."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4GIv4H_3Ce2F"
   },
   "source": [
    "##**Steps to Perform:**\n",
    "1. Install the Required Libraries and Dependencies\n",
    "2. Authenticate the Hugging Face Account and Set the API Key\n",
    "3. Use the Hugging Face Hub to Load the Flan T5 XXL model\n",
    "4. Create a Langchain HuggingFacePipeline for Text Generation\n",
    "5. Build a Chain Using Langchain\n",
    "6. Test and Run the Chain on Few a Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YMiJv-SkCo9e"
   },
   "source": [
    "###**Step 1: Install the Required Libraries and Dependencies**\n",
    "\n",
    "\n",
    "*   Install the necessary libraries, including **Langchain**, **Transformers**, and **Torch**.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "7Npz_xvrIxm3"
   },
   "outputs": [],
   "source": [
    "# !pip install langchain transformers torch accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ip2zy0mDC93N"
   },
   "source": [
    "###**Step 2: Authenticate the Hugging Face Account and Set the API Key**\n",
    "\n",
    "*   Click this link: https://huggingface.co/settings/tokens\n",
    "*   Login or create an account.\n",
    "*   Click on **New token**.\n",
    "*   On the dialog box, give a name and select the role as **write**.\n",
    "*   Copy the access token or API key.\n",
    "*   Replace **Your_HF_Key** with the copied key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d_my4ywipYl1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"<name>\"] = \"<value>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "veodvSN0Dc7r"
   },
   "source": [
    "###**Step 3: Use the Hugging Face Hub to Load the Flan T5 XXL model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "n4tjani-Zsd7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "# from langchain.llms import HuggingFacePipeline\n",
    "# from langchain import HuggingFaceHub\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\", temperature=1, max_length=512)\n",
    "# llm = HuggingFacePipeline(pipeline= generator)\n",
    "# llm=HuggingFaceHub(repo_id=\"google/flan-t5-xxl\", model_kwargs={\"temperature\":1, \"max_length\":512}, task=\"text-generation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wE90-mP9FHCJ"
   },
   "source": [
    "###**Step 4: Create a Langchain HuggingFacePipeline for Text Generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "4sfC8putZ4NX"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K6PN8DGuFSIn"
   },
   "source": [
    "###**Step 5: Build a Chain Using Langchain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "eMzZa8lmqF_i"
   },
   "outputs": [],
   "source": [
    "p1 = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step and tone should be {tone}\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=p1, input_variables=[\"question\", \"tone\"])\n",
    "\n",
    "llm_chain = prompt | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hCy0qU74Ff4p"
   },
   "source": [
    "###**Step 6: Test and Run the Chain on Few a Questions**\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "VzJK0rXNqKwA"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Question: Explain the concept of black holes in simple terms.\\n\\nAnswer: Let's think step by step and tone should be Professional Black Hole Design or something else you can think of. For now, I won't talk about the basic concepts of\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Question 1\n",
    "question = \"Explain the concept of black holes in simple terms.\"\n",
    "tone = \"Professional\"\n",
    "llm_chain.invoke({\"question\":question, \"tone\":tone})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z3-3arLF-1Ms"
   },
   "outputs": [],
   "source": [
    "#Question 2\n",
    "question = \"What are the main causes of climate change, and how can we address them?\"\n",
    "\n",
    "llm_chain.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UdK6ZB2GBMM0"
   },
   "outputs": [],
   "source": [
    "#Question 3\n",
    "question = \"Provide a brief overview of the history of artificial intelligence.\"\n",
    "\n",
    "llm_chain.run(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U openai langchain langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AzureChatOpenAI',\n",
       " 'AzureOpenAI',\n",
       " 'AzureOpenAIEmbeddings',\n",
       " 'ChatOpenAI',\n",
       " 'OpenAI',\n",
       " 'OpenAIEmbeddings',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " 'chat_models',\n",
       " 'custom_tool',\n",
       " 'embeddings',\n",
       " 'llms',\n",
       " 'tools']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import langchain_openai\n",
    "dir(langchain_openai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.1'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import langchain\n",
    "langchain.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "import os\n",
    "# os.environ[\"open_api_key\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gpt-4o-mini'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(template=\"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step and tone should be {tone}, answer= only 100 tokens\"\"\",\n",
    "               input_variables= [\"question\", \"tone\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 1\n",
    "question = \"Explain the concept of black holes in simple terms.\"\n",
    "tone = \"Professional\"\n",
    "result = chain.invoke({\"question\":question, \"tone\":tone})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A black hole is a region in space where gravity is so strong that nothing, not even light, can escape from it. This occurs when a massive star collapses under its own gravity after exhausting its nuclear fuel. The boundary around a black hole is called the event horizon; once something crosses this line, it cannot return. Black holes can vary in size, from small ones formed by dying stars to supermassive ones found at the centers of galaxies. They are fascinating objects that challenge our understanding of physics and the universe.\n"
     ]
    }
   ],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eGtdoqadFq-R"
   },
   "source": [
    "##**Conclusion**\n",
    "\n",
    "This sets the stage for your Langchain journey, allowing you to interact with language models seamlessly. In the upcoming demos, we will explore more advanced applications development with Langchain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 [3.10]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
